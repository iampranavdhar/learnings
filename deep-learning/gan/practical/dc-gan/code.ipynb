{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCGAN Implementation\n",
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,img_channels,features_d):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            #Input: N x img_channels x 64 x 64\n",
    "            nn.Conv2d(img_channels,features_d,kernel_size=4,stride=2,padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # _block(in_channels,out_channels,kernel_size,stride,padding)\n",
    "            self._block(features_d,features_d*2,4,2,1),\n",
    "            self._block(features_d*2,features_d*4,4,2,1),\n",
    "            self._block(features_d*4,features_d*8,4,2,1),\n",
    "            nn.Conv2d(features_d*8,1,kernel_size=4,stride=2,padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def _block(self,in_channels,out_channels,kernel_size,stride,padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels,out_channels,kernel_size,stride,padding,bias=False),\n",
    "            nn.BatchNorm2d(out_channels,affine=True),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.disc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Explaining the Discriminator\n",
    "\n",
    "input: N x img_channels x 64 x 64\n",
    "why 64x64? because we are using celebA dataset which is 64x64 and 3 channels (RGB), hence img_channels = 3, N is the batch size\n",
    "\n",
    "output: N x 1 x 1 x 1\n",
    "why 1x1x1? because we are using Conv2d with kernel_size=4, stride=2, padding=1, hence the image size is reduced by 2 in each dimension\n",
    "hence 64 -> 32 -> 16 -> 8 -> 4 -> 2 -> 1\n",
    "hence the output is N x 1 x 1 x 1\n",
    "This is used to classify whether the input image is real or fake\n",
    "\n",
    "As per the DCGAN paper, we use LeakyReLU with slope 0.2 and we use BatchNorm2d after each Conv2d layer except the last one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self,z_dim,img_channels,features_g):\n",
    "        super(Generator,self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            #Input: N x z_dim x 1 x 1\n",
    "            self._block(z_dim,features_g*16,4,1,0),\n",
    "            self._block(features_g*16,features_g*8,4,2,1),\n",
    "            self._block(features_g*8,features_g*4,4,2,1),\n",
    "            self._block(features_g*4,features_g*2,4,2,1),\n",
    "            nn.ConvTranspose2d(features_g*2,img_channels,kernel_size=4,stride=2,padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def _block(self,in_channels,out_channels,kernel_size,stride,padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels,out_channels,kernel_size,stride,padding,bias=False),\n",
    "            nn.BatchNorm2d(out_channels,affine=True),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.gen(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Explaining the Generator\n",
    "\n",
    "1. Some info:\n",
    "   * input: N x z_dim\n",
    "   * output: N x img_channels x 64 x 64\n",
    "\n",
    "   * We start with a linear layer to convert the input noise vector of size z_dim to a tensor of size 4x4xfeatures_g*16 (4x4 image with features_g*16 channels)\n",
    "   * Then we use ConvTranspose2d to increase the image size by 2 in each dimension\n",
    "   * We use BatchNorm2d after each ConvTranspose2d layer\n",
    "   * We use ReLU activation function after each BatchNorm2d layer except the last one\n",
    "   * The final layer uses ConvTranspose2d with kernel_size=4, stride=2, padding=1 to generate an image of size 64x64\n",
    "   * In this case we use Tanh activation function so that the pixel values are in the range of -1 to 1\n",
    "   * z_dim is the input noise vector size\n",
    "\n",
    "2. What is the difference between Conv2d and ConvTranspose2d?<br>\n",
    "    Conv2d is used to reduce the image size (downsampling) and ConvTranspose2d is used to increase the image size (upsampling)\n",
    "\n",
    "3. What is the difference between BatchNorm1d and BatchNorm2d?<br>\n",
    "    BatchNorm1d is used for 1D data like time series data and BatchNorm2d is used for 2D data like images\n",
    "\n",
    "4. What is the difference between ReLU and LeakyReLU?<br>\n",
    "    ReLU is Rectified Linear Unit which is max(0,x) and LeakyReLU is max(0.01x,x) or max(0.2x,x) where x is the input to the activation function (output of the linear layer)\n",
    "\n",
    "5. What does strided convolution layers mean ?<br>\n",
    "    Strided convolution layers refer to convolutional layers where the filter/kernel is applied to the input data with a certain step size or \"stride\" greater than 1. In a standard convolution operation, the filter slides over the input one pixel at a time, but with strided convolution, the filter moves multiple pixels at a time.\n",
    "\n",
    "    Here's how it works:\n",
    "\n",
    "    Standard Convolution: In a standard convolution operation, the filter/kernel moves across the input data one pixel at a time in both the horizontal and vertical directions. This is often referred to as a stride of 1.\n",
    "\n",
    "    Strided Convolution: In a strided convolution, the filter/kernel moves across the input with a larger step size. For example, a stride of 2 means the filter moves 2 pixels at a time instead of 1. This results in a reduction of the spatial dimensions of the output feature map compared to the input.\n",
    "\n",
    "    The main effect of using strided convolution layers is that they can help reduce the spatial dimensions of the data, effectively downsampling it. This can be useful in convolutional neural network (CNN) architectures for tasks like image classification, where reducing the spatial dimensions gradually through layers can help capture hierarchical features at different scales while reducing computational complexity.\n",
    "\n",
    "    However, it's important to note that using strided convolutions can also result in information loss, as some of the input data may not be considered when applying the filter. Therefore, the choice of stride size needs to be carefully considered based on the specific task and architecture requirements.\n",
    "\n",
    "\n",
    "6. What is batch normalization? <br>\n",
    "\n",
    "    Batch normalization is a technique used in neural networks to normalize the input data to each layer, typically by adjusting and scaling the activations. The goal of batch normalization is to improve the training stability and speed of deep neural networks by reducing internal covariate shift.\n",
    "\n",
    "    Here's how it works:\n",
    "\n",
    "    1. Normalization: In batch normalization, the input data to each layer is normalized by subtracting the mean and dividing by the standard deviation of the batch of data. This helps to center and scale the data, making it more stable and easier to train.\n",
    "    2. Scaling and Shifting: After normalization, the data is scaled and shifted by learnable parameters (gamma and beta) to allow the network to learn the optimal scale and shift for each layer.\n",
    "    3. Batch Statistics: During training, batch normalization calculates the mean and standard deviation of the input data for each mini-batch. These batch statistics are used to normalize the data and update the running averages of the mean and standard deviation for inference.\n",
    "    4. Regularization: Batch normalization acts as a form of regularization by adding noise to the input data, which can help prevent overfitting and improve generalization.\n",
    "    5. Activation Function: Batch normalization is typically applied before the activation function in a neural network, helping to stabilize the activations and improve the training process.\n",
    "    6. Training and Inference: During training, batch normalization uses the batch statistics to normalize the data. During inference, the running averages of the mean and standard deviation are used to normalize the data.\n",
    "\n",
    "\n",
    "7. What is convtranspose? <br>\n",
    "\n",
    "    A Convolutional Transpose, also known as a Deconvolution or a Transposed Convolution, is an operation often used in neural networks, especially in architectures like autoencoders and generative models such as variational autoencoders (VAEs) and generative adversarial networks (GANs).\n",
    "\n",
    "    In a regular convolution operation, you apply a filter/kernel to the input data to produce a feature map, reducing the spatial dimensions of the input (e.g., going from a larger image to a smaller feature map). Convolutional transpose, on the other hand, performs the opposite operation â€“ it upsamples the input data, increasing its spatial dimensions.\n",
    "\n",
    "    Here's how it works:\n",
    "\n",
    "    ConvTranspose operation involves zero-padding the input and then applying a convolution operation using a kernel that's larger than the original input.\n",
    "\n",
    "    The convolution operation is applied with a certain stride, which determines how much the kernel shifts over the input.\n",
    "\n",
    "    This process effectively expands the spatial dimensions of the input, producing an output with a larger spatial extent.\n",
    "\n",
    "    ConvTranspose layers are often used in decoder parts of autoencoder architectures to reconstruct data from the learned latent space or in generator parts of GANs to upsample noise or low-resolution data into high-resolution images.\n",
    "\n",
    "    The term \"transpose\" in ConvTranspose comes from the notion of transposing the convolution operation, i.e., it's like doing the reverse of a convolution. However, it's important to note that the operation itself isn't a true mathematical transpose; it's more about reversing the effects of a convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "lr = 2e-4\n",
    "z_dim = 100\n",
    "image_dim = 64\n",
    "image_channels = 3\n",
    "features_d = 64\n",
    "features_g = 64\n",
    "batch_size = 64\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset download and preprocessing\n",
    "fixed_noise = torch.randn((batch_size,z_dim,1,1)).to(device)\n",
    "transforms = transforms.Compose([\n",
    "    transforms.Resize(image_dim),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5 for _ in range(image_channels)],[0.5 for _ in range(image_channels)])\n",
    "])\n",
    "\n",
    "dataset = datasets.CIFAR10(root='dataset/',train=True,transform=transforms,download=True)\n",
    "loader = DataLoader(dataset,batch_size=batch_size,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Discriminator and Generator\n",
    "disc = Discriminator(image_channels,features_d).to(device)\n",
    "gen = Generator(z_dim,image_channels,features_g).to(device)\n",
    "\n",
    "# Weight initialization\n",
    "init_weights(disc)\n",
    "init_weights(gen)\n",
    "\n",
    "# Initialize the optimizers\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "# Loss\n",
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the role of weight initialization?<br>\n",
    "    Weight initialization is important because it helps in training the model faster and more effectively by preventing the gradients from vanishing or exploding during backpropagation, this means that the model can learn the patterns in the data more effectively\n",
    "<br>\n",
    "\n",
    "2. What is this rule of mean 0 and standard deviation 1?<br>\n",
    "    The \"rule\" of initializing weights with a mean of 0 and a standard deviation of 1 is a common practice in neural network weight initialization, particularly for certain types of layers like fully connected (dense) layers. This rule is also known as \"Xavier initialization\" or \"Glorot initialization,\" named after the researchers who introduced it.\n",
    "\n",
    "    Here's an explanation of why this rule is used and its significance:\n",
    "\n",
    "    1. **Stabilizing Gradients**: Initializing weights with a mean of 0 helps ensure that the initial outputs of neurons are centered around 0. This can help stabilize the gradients during backpropagation, preventing them from exploding or vanishing as they propagate through the network layers.\n",
    "\n",
    "    2. **Balancing Signal Propagation**: By setting the standard deviation to 1, the weights are initially scaled to have a moderate range of values. This helps balance the magnitude of the signal propagated forward through the network, preventing it from becoming too large or too small as it passes through multiple layers.\n",
    "\n",
    "    3. **Improving Training Dynamics**: Proper weight initialization can lead to more stable and efficient training dynamics. It can help networks converge faster and achieve better generalization performance by ensuring that the weights are initialized in a way that facilitates effective learning.\n",
    "\n",
    "    4. **Applicability to Different Layers**: While initializing weights with mean 0 and standard deviation 1 is commonly used for fully connected layers, variations of this rule exist for other types of layers, such as convolutional and recurrent layers. For example, Xavier initialization adapts the initialization scheme based on the number of input and output units of a layer to account for differences in signal propagation.\n",
    "\n",
    "    Overall, initializing weights with a mean of 0 and a standard deviation of 1 is a widely adopted practice in neural network initialization because it helps address common issues related to gradient stability and signal propagation during training, contributing to more effective and efficient learning.\n",
    "<br>\n",
    "\n",
    "3. Why are we using standard deviation 0.02 for weight initialization?<br>\n",
    "    This is a common practice in GANs to prevent the generator from collapsing all the generated images to a single point in the image space (mode collapse) and this is recommended in the DCGAN paper\n",
    "\n",
    "4. But why is Xaviers initialization not used in GANs?<br>\n",
    "    Xavier initialization is not used in GANs because it is designed for feedforward neural networks and may not be optimal for GANs, which have a different architecture and training dynamics. In GANs, the goal is to train a generator and a discriminator simultaneously, and the training process involves a minimax game between the two networks. The dynamics of GAN training, such as the competition between the generator and discriminator, can lead to different requirements for weight initialization compared to standard feedforward networks.\n",
    "\n",
    "    In GANs, the choice of weight initialization can have a significant impact on the training dynamics and the quality of the generated samples. For example, using Xavier initialization in GANs may lead to issues such as mode collapse, where the generator produces limited and repetitive samples, or training instability, where the networks fail to converge to a stable equilibrium.\n",
    "\n",
    "    Instead of Xavier initialization, GANs often use different weight initialization strategies that are tailored to the specific requirements of GAN training. For example, in DCGAN (Deep Convolutional GAN), a popular architecture for image generation, the weights of the generator and discriminator are initialized with a mean of 0 and a standard deviation of 0.02. This initialization scheme is chosen to balance the signal propagation and prevent mode collapse, helping the networks learn diverse and realistic image distributions.\n",
    "\n",
    "    In summary, while Xavier initialization is a common practice for feedforward neural networks, it may not be suitable for GANs due to their unique architecture and training dynamics. GANs often require specialized weight initialization strategies to address challenges such as mode collapse and training instability, leading to improved performance and sample quality.\n",
    "\n",
    "5. Then what is Kaiming initialization?<br>\n",
    "    Kaiming initialization, also known as He initialization, is a weight initialization technique that is specifically designed for deep neural networks with rectified linear units (ReLUs) as activation functions. It addresses the issue of vanishing or exploding gradients that can occur during training of deep networks by initializing the weights in a way that helps stabilize the gradients and improve learning.\n",
    "\n",
    "    Here are some key points about Kaiming initialization:\n",
    "\n",
    "    1. **Designed for ReLU Activations**: Kaiming initialization is tailored for networks that use ReLU activation functions, which are commonly used in deep learning models due to their ability to mitigate the vanishing gradient problem. By initializing the weights in a way that accounts for the characteristics of ReLU activations, Kaiming initialization helps ensure that the gradients remain stable during training.\n",
    "\n",
    "    2. **Mean and Variance Adjustment**: Kaiming initialization sets the mean of the weights to 0 and adjusts the variance based on the type of activation function used. For ReLU activations, the variance is scaled by a factor that depends on the number of input units to the layer, helping to balance the signal propagation and prevent the gradients from vanishing or exploding.\n",
    "\n",
    "    3. **Improves Training Dynamics**: By initializing the weights in a way that is tailored to ReLU activations, Kaiming initialization can improve the training dynamics of deep networks. It helps networks converge faster, learn more effectively, and achieve better generalization performance by ensuring that the gradients are stable and the weights are initialized in a way that facilitates learning.\n",
    "\n",
    "    4. **Applicability to Deep Networks**: Kaiming initialization is particularly well-suited for deep neural networks with many layers, where the vanishing gradient problem can be a significant challenge. By providing a principled way to initialize the weights based on the characteristics of ReLU activations, Kaiming initialization helps deep networks train more effectively and achieve better performance.\n",
    "\n",
    "    In summary, Kaiming initialization is a weight initialization technique that is specifically designed for deep neural networks with ReLU activations. By setting the mean of the weights to 0 and adjusting the variance based on the type of activation function, Kaiming initialization helps stabilize the gradients and improve the training dynamics of deep networks, leading to more effective learning and better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values = {\n",
    "    'disc': [],\n",
    "    'gen': []\n",
    "}\n",
    "\n",
    "# Set the models to training mode\n",
    "disc.train()\n",
    "gen.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx,(real,_) in enumerate(loader):\n",
    "        real = real.to(device)\n",
    "        noise = torch.randn((batch_size,z_dim,1,1)).to(device)\n",
    "        fake = gen(noise)\n",
    "        \n",
    "        disc_real = disc(real).view(-1)\n",
    "        lossD_real = loss_fn(disc_real,torch.ones_like(disc_real))\n",
    "        disc_fake = disc(fake).view(-1)\n",
    "        lossD_fake = loss_fn(disc_fake,torch.zeros_like(disc_fake))\n",
    "        lossD = (lossD_real + lossD_fake)/2\n",
    "        disc.zero_grad()\n",
    "        lossD.backward(retain_graph=True)\n",
    "        opt_disc.step()\n",
    "        \n",
    "        output = disc(fake).view(-1)\n",
    "        lossG = loss_fn(output,torch.ones_like(output))\n",
    "        gen.zero_grad()\n",
    "        lossG.backward()\n",
    "        opt_gen.step()\n",
    "        \n",
    "        if batch_idx == 0:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader)} Loss D: {lossD:.4f}, Loss G: {lossG:.4f}\")\n",
    "            loss_values['disc'].append(lossD.item())\n",
    "            loss_values['gen'].append(lossG.item())\n",
    "            with torch.no_grad():\n",
    "                fake = gen(fixed_noise)\n",
    "                img_grid_real = real[:32]\n",
    "                img_grid_fake = fake[:32]\n",
    "                img_grid_real = torchvision.utils.make_grid(img_grid_real,normalize=True).cpu()\n",
    "                img_grid_fake = torchvision.utils.make_grid(img_grid_fake,normalize=True).cpu()\n",
    "\n",
    "                plt.imshow(np.transpose(img_grid_real.numpy(),(1,2,0)))\n",
    "                plt.show()\n",
    "                plt.imshow(np.transpose(img_grid_fake.numpy(),(1,2,0)))\n",
    "                plt.show()\n",
    "\n",
    "# Save the model\n",
    "torch.save(gen.state_dict(), 'gen.pth')\n",
    "torch.save(disc.state_dict(), 'disc.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss values\n",
    "plt.plot(loss_values['disc'], label='Discriminator')\n",
    "plt.plot(loss_values['gen'], label='Generator')\n",
    "plt.title('Loss Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
