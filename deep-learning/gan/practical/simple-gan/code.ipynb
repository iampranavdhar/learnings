{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Linear(img_dim, 128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.disc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the difference between nn.LeakyReLU and nn.ReLU?\n",
    "nn.LeakyReLU allows a small gradient when the unit is not active (i.e. x < 0) while nn.ReLU has a gradient of 0 when x < 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, img_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.Linear(z_dim, 256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(256, img_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gen(x)\n",
    "    \n",
    "# Why are we using tanh in the generator and sigmoid in the discriminator?\n",
    "# The tanh function outputs values between -1 and 1, which is the range of the MNIST images. \n",
    "# The sigmoid function outputs values between 0 and 1, which is the range of the discriminator output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "lr = 5e-4 # learning rate, this is the best as of now and suggested by multiple papers and Andrej Karpathy\n",
    "z_dim = 128 # latent dim for generator, this is used to generate random noise which is then used to generate images\n",
    "img_dim = 28 * 28 * 1\n",
    "batch_size = 16\n",
    "num_epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the models\n",
    "disc = Discriminator(img_dim).to(device)\n",
    "gen = Generator(z_dim, img_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise\n",
    "# Why do we need noise?\n",
    "# The noise is used as an input to the generator to create a fake image. This noise is sampled from a normal distribution.\n",
    "fixed_noise = torch.randn((batch_size, z_dim)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the images to be between -1 and 1\n",
    "transforms = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is part of the PyTorch library, specifically the torchvision.transforms module. It's used to define a sequence of transformations to be applied to an input image or data before feeding it into a neural network for processing. Let's break down what each part of this function does:\n",
    "\n",
    "transforms.Compose: This function combines several transformations into a single transformation pipeline. It takes a list of transformations as input and applies them sequentially to the input data.\n",
    "\n",
    "transforms.ToTensor(): This transformation converts input data (such as images) into PyTorch tensors. It's commonly used because neural networks typically operate on tensors rather than raw image data.\n",
    "\n",
    "transforms.Normalize((0.5,), (0.5,)): This transformation normalizes the tensor by subtracting the mean and dividing by the standard deviation. In this case, it subtracts 0.5 from each pixel value and then divides by 0.5. This effectively scales the pixel values to be between -1 and 1, which is a common practice in deep learning to make training more stable.\n",
    "\n",
    "So, in summary, the transforms.Compose function defines a transformation pipeline that converts input data into tensors and then normalizes those tensors. This pipeline is often used when preprocessing images or other data for input into neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and Load the dataset\n",
    "dataset = datasets.MNIST(root=\"dataset/\", transform=transforms, download=True)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers and loss function\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=lr)\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=lr)\n",
    "\n",
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_fake = SummaryWriter(f\"runs/GAN_MNIST/fake\")\n",
    "writer_real = SummaryWriter(f\"runs/GAN_MNIST/real\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of Generative Adversarial Networks (GANs) training on the MNIST dataset, these lines create two SummaryWriter objects, one for logging generated (fake) images and another for logging real images. The SummaryWriter is a utility provided by PyTorch's TensorBoard integration for logging various metrics and visualizations during model training.\n",
    "\n",
    "Let's break down what each line does:\n",
    "\n",
    "writer_fake = SummaryWriter(f\"runs/GAN_MNIST/fake\"): This line creates a SummaryWriter object named writer_fake. It specifies a directory path runs/GAN_MNIST/fake where the logs for fake images will be stored. Typically, during GAN training, the generator network produces fake images, and you might want to visualize these images over time to see how the generator improves.\n",
    "\n",
    "writer_real = SummaryWriter(f\"runs/GAN_MNIST/real\"): Similarly, this line creates another SummaryWriter object named writer_real, but this time it's for logging real images. In the context of GAN training, real images are the ones sampled from the MNIST dataset that serve as the ground truth for the discriminator network. Logging real images can help monitor how well the discriminator distinguishes between real and fake images.\n",
    "\n",
    "By using these SummaryWriter objects, you can log various information such as images, scalar values (e.g., loss), histograms, and more during the training process. This information can then be visualized in TensorBoard to gain insights into the training progress and the performance of the GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Training\"):\n",
    "    for batch_idx, (real, _) in enumerate(loader):\n",
    "        real = real.view(-1, 784).to(device)\n",
    "        batch_size = real.shape[0]\n",
    "\n",
    "        ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "        noise = torch.randn(batch_size, z_dim).to(device)\n",
    "        fake = gen(noise)\n",
    "        disc_real = disc(real).view(-1)\n",
    "        loss_disc_real = loss_fn(disc_real, torch.ones_like(disc_real)) # first parameter is prediction and second is target\n",
    "        disc_fake = disc(fake).view(-1)\n",
    "        loss_disc_fake = loss_fn(disc_fake, torch.zeros_like(disc_fake)) # first parameter is prediction and second is target\n",
    "        loss_disc = (loss_disc_real + loss_disc_fake) / 2 # average of both the losses\n",
    "        disc.zero_grad()\n",
    "        loss_disc.backward(retain_graph=True) # retain_graph=True is used to retain the computational graph so that we can call backward() again, here we are using we want to use fake again\n",
    "        opt_disc.step()\n",
    "\n",
    "        ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "        output = disc(fake).view(-1)\n",
    "        loss_gen = loss_fn(output, torch.ones_like(output))\n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        # Print losses occasionally and print to tensorboard\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader)} \\\n",
    "                  Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gen(fixed_noise).reshape(-1, 1, 28, 28)\n",
    "                data = real.reshape(-1, 1, 28, 28)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake, normalize=True)\n",
    "                img_grid_real = torchvision.utils.make_grid(data, normalize=True)\n",
    "\n",
    "                writer_fake.add_image(\"Mnist Fake Images\", img_grid_fake, global_step=step)\n",
    "                writer_real.add_image(\"Mnist Real Images\", img_grid_real, global_step=step)\n",
    "\n",
    "                step += 1\n",
    "\n",
    "# Save the model\n",
    "torch.save(gen.state_dict(), \"gen.pth\")\n",
    "torch.save(disc.state_dict(), \"disc.pth\")\n",
    "\n",
    "# Close the tensorboard writer\n",
    "writer_fake.close()\n",
    "writer_real.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results on changing hyperparameters:\n",
    "1. lr = 5e-4, z_dim = 64, img_dim = 28 * 28 * 1, batch_size = 16, num_epochs = 25\n",
    "   1. Results: D Loss = 0.5637 | G Loss = 0.9129\n",
    "2. lr = 7e-4, z_dim = 32, img_dim = 28 * 28 * 1, batch_size = 16, num_epochs = 25\n",
    "   1. Results: D Loss = 0.4870 | G Loss = 1.0671\n",
    "3. lr = 3e-4, z_dim = 64, img_dim = 28 * 28 * 1, batch_size = 32, num_epochs = 25\n",
    "   1. Results: D Loss = 0.5484 | G Loss = 0.9143\n",
    "4. lr = 3e-4, z_dim = 64, img_dim = 28 * 28 * 1, batch_size = 16, num_epochs = 30\n",
    "   1. Results: D Loss = 0.5484 | G Loss = 0.9143\n",
    "\n",
    "Results: 2 > 1 > 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "How to improve the GAN?\n",
    "1. Use different architectures for the generator and discriminator (e.g. DCGAN, WGAN, etc.)\n",
    "2. Use different hyperparameters, one thing to note is that GANs are very sensitive to hyperparameters\n",
    "3. Use different loss functions\n",
    "4. Train for longer\n",
    "5. Add regularization techniques (e.g. weight clipping in WGAN)\n",
    "6. Use different types of normalization\n",
    "7. Use different types of noise\n",
    "8. Use different types of optimizers\n",
    "\n",
    "What is DCGAN?<br>\n",
    "DCGAN stands for Deep Convolutional Generative Adversarial Network. It is a type of GAN that uses convolutional layers in both the generator and discriminator.\n",
    "This allows the model to learn more complex patterns in the data and generate higher quality images. DCGANs are commonly used for image generation tasks and have been shown to produce realistic images in a variety of domains.\n",
    "\n",
    "What is WGAN?<br>\n",
    "WGAN stands for Wasserstein Generative Adversarial Network. It is a type of GAN that uses the Wasserstein distance as the loss function instead of the traditional binary cross-entropy loss. And this is used to stabilize the training of GANs and produce higher quality images. WGANs have been shown to be more stable and produce better results than traditional GANs in many cases.\n",
    "\n",
    "What is weight clipping?<br>\n",
    "Weight clipping is a regularization technique used in WGANs to enforce a Lipschitz constraint on the discriminator. This involves clipping the weights of the discriminator to a small range after each training step. This helps to prevent the discriminator from becoming too powerful and dominating the training process, which can lead to mode collapse and other issues. Weight clipping has been shown to improve the stability and performance of WGANs in practice.\n",
    "\n",
    "How is weight clipping different from learning rate?<br>\n",
    "Weight clipping is a regularization technique that is applied to the weights of the discriminator in a GAN. It involves clipping the weights to a small range after each training step. This helps to prevent the discriminator from becoming too powerful and dominating the training process. Learning rate, on the other hand, is a hyperparameter that controls how much the weights of the model are updated during training. It determines the size of the steps taken in the direction of the gradient during optimization. Both weight clipping and learning rate are important hyperparameters that can affect the performance of a GAN, but they serve different purposes and are applied in different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the generator\n",
    "with torch.no_grad():\n",
    "    noise = torch.randn(9, z_dim).to(device)\n",
    "    img = gen(noise).view(-1, 1, 28, 28)\n",
    "    img_grid = torchvision.utils.make_grid(img, nrow=3, normalize=True)\n",
    "    plt.imshow(img_grid.permute(1, 2, 0).cpu().numpy())\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
