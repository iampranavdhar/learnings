{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Adversial Neural Network (CGAN)\n",
    "\n",
    "Till now we have seen how to generate images using GANs. But what if we want to generate images of a specific type? For example, we want to generate images of a specific digit. In such cases, we can use Conditional GANs (CGANs). In CGANs, we provide the generator and discriminator with additional information, called the condition, which helps them to generate images of a specific type.\n",
    "\n",
    "For example let's say we want to generate images of digit 5. In this case, we can provide the generator and discriminator with the label of digit 5. The generator will generate images of digit 5 and the discriminator will classify the images as real or fake.\n",
    "\n",
    "How we will provide the label to the generator and discriminator? We will concatenate the label with the noise vector and provide it as input to the generator. Similarly, we will concatenate the label with the image and provide it as input to the discriminator. \n",
    "\n",
    "And label is converted to one-hot encoded vector. For example, if the label is 5, then it will be converted to [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]. \n",
    "\n",
    "And we use nn.Embedding layer to represent the label. nn.Embedding is similar to nn.Linear layer. It takes the label as input and returns the corresponding embedding vector.\n",
    "\n",
    "For example, if the label is 5, then the embedding layer will return the embedding vector of 5. This embedding vector is then concatenated with the noise vector and provided as input to the generator. Similarly, the embedding vector is concatenated with the image and provided as input to the discriminator. \n",
    "\n",
    "Example:\n",
    "- Generator input: [noise vector, label]\n",
    "- Discriminator input: [image, label]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Embedding in PyTorch is a layer that maps integer indices to dense vectors of fixed size (embeddings). Each integer index corresponds to a unique embedding vector.\n",
    "\n",
    "When you're using nn.Embedding in the context of CGANs, you're essentially providing the label (which is typically represented as an integer index) to the nn.Embedding layer, and it returns the corresponding embedding vector for that label.\n",
    "\n",
    "So, to correct the statement: nn.Embedding provides the embedding vector, not the label itself. When you pass a label (as an integer index) to the nn.Embedding layer, it returns the embedding vector associated with that label.\n",
    "\n",
    "For example, if you have a label \"5\" which corresponds to the integer index 5, you pass this index to the nn.Embedding layer, and it returns the embedding vector for the label \"5\". This embedding vector is then concatenated with other inputs (like noise vector for the generator or image for the discriminator) in the CGAN architecture.\n",
    "\n",
    "In summary, nn.Embedding is used to map integer indices (labels) to dense embedding vectors in CGANs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the context of an embedding layer like nn.Embedding in PyTorch, the embedding size refers to the dimensionality of the dense embedding vectors that represent each label.\n",
    "\n",
    "When you define the embedding layer, you specify the number of classes (or unique labels) and the size of the embedding vectors. The size of the embedding vectors is a hyperparameter that you set based on the complexity of the problem and the size of the embedding space you want to create.\n",
    "\n",
    "For example, in the code snippet provided:\n",
    "\n",
    "embedding_size = 100\n",
    "embedding_layer = nn.Embedding(num_classes, embedding_size)\n",
    "\n",
    "Here, embedding_size is set to 100, which means each label will be represented by a dense vector of size 100. This means that the embedding layer will learn a 100-dimensional embedding space where each dimension captures certain features or characteristics of the corresponding label.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self,latent_dim=100,img_channels=1,num_classes=10,hidden_dim=64):\n",
    "        \"\"\"\n",
    "        Explanation:\n",
    "        What is the use of latent_dim, img_channels and hidden_dim?\n",
    "        - latent_dim: The dimension of the latent space. This is the dimension of the input to the generator. This is the dimension of the noise vector that is input to the generator. This is a hyperparameter that can be tuned to get better results.\n",
    "        - img_channels: The number of channels in the input image. This is the number of channels in the input image that is to be generated by the generator. This is based on the dataset that is being used. For example, for grayscale images, this will be 1 and for RGB images, this will be 3.\n",
    "        - hidden_dim: The number of hidden units in the generator. This is the number of hidden units in the generator that are used to generate the output image. This is a hyperparameter that can be tuned to get better results.\n",
    "        \"\"\"\n",
    "        super(Generator,self).__init__()\n",
    "        self.latent_dim = latent_dim # We are adding num_classes to the latent_dim to get the final latent_dim. This is done because we are adding the class information to the noise vector.\n",
    "        self.gen_model = nn.Sequential(\n",
    "            self._gen_block(self.latent_dim,hidden_dim*4), # Let's say the dimension of the input noise vector is 100 and the hidden_dim is 64. Then, the output of this layer will be of size 64*4=256.\n",
    "            self._gen_block(hidden_dim*4,hidden_dim*2,kernel_size=4,stride=1),\n",
    "            self._gen_block(hidden_dim*2,hidden_dim),\n",
    "            self._gen_block(hidden_dim,img_channels,kernel_size=4,final_layer=True)\n",
    "        )\n",
    "\n",
    "    def _gen_block(self,input_channels,output_channels,kernel_size=3,stride=2,final_layer=False): \n",
    "        \"\"\"\n",
    "        Explanation:\n",
    "        What is the use of kernel_size and stride?\n",
    "        - kernel_size: The size of the kernel to be used in the convolution operation. This is the size of the filter that will be applied to the input image.\n",
    "        - stride: The stride of the convolution operation. This is the number of pixels by which the kernel moves in each step. In case of ConvTranspose2d, this is the number of times by which the output image dimensions will be increased. For example, if the stride is 2, the output image dimensions will be twice the input image dimensions.\n",
    "        \"\"\"\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels,output_channels,kernel_size,stride),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.ReLU(inplace=True) # inplace=True means that the operation will be done in place, i.e., the output will be stored in the same memory location as the input. This is done to save memory.\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels,output_channels,kernel_size,stride),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "\n",
    "    def forward(self,noise):\n",
    "        \"\"\"\n",
    "        Explanation:\n",
    "        Forward pass through the generator network.\n",
    "        We are reshaping the noise vector to a 4D tensor of shape (batch_size,latent_dim,1,1) so that it can be passed through the generator network.\n",
    "        Why are we reshaping the noise vector to a 4D tensor?\n",
    "        - The input to the generator is a 4D tensor of shape (batch_size,channels,height,width). So, we need to reshape the noise vector to a 4D tensor so that it can be passed through the generator network.\n",
    "\n",
    "        Why are height and width 1?\n",
    "        - The height and width are 1 because the input to the generator is a noise vector. So, the height and width are 1.\n",
    "        \"\"\"\n",
    "        x = noise.view(len(noise),self.latent_dim,1,1)\n",
    "        return self.gen_model(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why did we use stride 1 initially and then changed to 2?\n",
    "\n",
    "The choice of stride in convolutional layers affects the spatial dimensions of the feature maps produced after convolution. Here's why the stride is set to 1 initially and then changed to 2 in subsequent layers in the generator network:\n",
    "\n",
    "Initial Layer with Stride 1:\n",
    "When the stride is set to 1, the convolutional operation moves the filter/kernel one pixel at a time across the input feature map. This results in output feature maps with the same spatial dimensions as the input feature maps but potentially with different channel dimensions due to the number of output filters.\n",
    "Setting the stride to 1 in the initial layer helps preserve the spatial information of the input noise vector. Since the initial noise vector typically contains low-level features or patterns that are important for generating the overall structure of the output image, preserving spatial details in the early stages of generation can be beneficial.\n",
    "Subsequent Layers with Stride 2:\n",
    "As the generator network progresses through its layers, it aims to learn increasingly abstract and high-level features. Using a stride of 2 in subsequent layers reduces the spatial dimensions of the feature maps while increasing the number of channels, effectively \"zooming out\" and capturing broader patterns and structures in the data.\n",
    "By reducing the spatial dimensions with larger strides, the generator can focus on capturing higher-level semantic information and global context, which are crucial for generating coherent and realistic images.\n",
    "Additionally, increasing the stride in deeper layers can help control the growth of computational complexity and memory requirements, as it reduces the spatial resolution of feature maps and hence the number of parameters in subsequent layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,img_channels=1,num_classes=10,hidden_dim=64):\n",
    "        \"\"\"\n",
    "        Explanation:\n",
    "        What is the use of img_channels and hidden_dim?\n",
    "        - img_channels: The number of channels in the input image. This is the number of channels in the input image that is to be generated by the generator. This is based on the dataset that is being used. For example, for grayscale images, this will be 1 and for RGB images, this will be 3.\n",
    "        - hidden_dim: The number of hidden units in the discriminator. This is the number of hidden units in the discriminator that are used to classify the input image as real or fake. This is a hyperparameter that can be tuned to get better results.\n",
    "        \"\"\"\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.img_channels = img_channels # We are adding num_classes to the img_channels to get the final img_channels. This is done because we are adding the class information to the input image.\n",
    "        self.disc_model = nn.Sequential(\n",
    "            self._disc_block(self.img_channels,hidden_dim), # Let's say the dimension of the input image is 1 and the hidden_dim is 64. Then, the output of this layer will be of size 64.\n",
    "            self._disc_block(hidden_dim,hidden_dim*2,), # The output of this layer will be of size 64*2=128.\n",
    "            self._disc_block(hidden_dim*2,1,final_layer=True), # The output of this layer will be of size 1.\n",
    "        )\n",
    "\n",
    "    def _disc_block(self,input_channels,output_channels,kernel_size=4,stride=2, final_layer = False):\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels,output_channels,kernel_size,stride),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.LeakyReLU(0.2,inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels,output_channels,kernel_size,stride),\n",
    "            )\n",
    "        \n",
    "    def forward(self,image_vector):\n",
    "        \"\"\"\n",
    "        Explanation:\n",
    "        Forward pass through the discriminator network.\n",
    "        \"\"\"\n",
    "        disc_output = self.disc_model(image_vector)\n",
    "        return disc_output.view(len(disc_output),-1)\n",
    "        # The output of the discriminator is a 2D tensor of shape (batch_size,1). So, we are reshaping it to a 1D tensor of shape (batch_size,) so that it can be used to calculate the loss.\n",
    "        # How is output of the discriminator a 2D tensor of shape (batch_size,1)?\n",
    "        # - The output of the discriminator is a 2D tensor of shape (batch_size,1) because the discriminator is a binary classifier. It classifies the input image as real or fake. So, the output will be a 2D tensor of shape (batch_size,1) where each element in the tensor will be the probability of the input image being real or fake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_noise_vector(batch_size,latent_dim,device):\n",
    "    \"\"\"\n",
    "    Explanation:\n",
    "    This function is used to create a noise vector of size (batch_size,latent_dim,1,1) that is used as input to the generator.\n",
    "    \"\"\"\n",
    "    return torch.randn(batch_size,latent_dim,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "def plot_images_from_tensor(tensor, num_images=25, size=(1, 28, 28), nrow=5, show=True):\n",
    "    \"\"\"\n",
    "    Explanation:\n",
    "    This function is used to plot the images generated by the generator.\n",
    "    \"\"\"\n",
    "    tensor = tensor.detach().cpu() # detach() is used to remove the tensor from the computation graph. This is done to save memory.\n",
    "    tensor = tensor.view(-1, *size) # .view() is used to reshape the tensor to the specified dimensions.\n",
    "    grid = make_grid(tensor, nrow=nrow) # make_grid() is used to create a grid of images.\n",
    "    plt.figure(figsize=(10, 10)) # figsize is used to set the size of the figure.\n",
    "    plt.imshow(grid.permute(1, 2, 0)) # .permute() is used to change the dimensions of the tensor. Here, we are changing the dimensions from (C, H, W) to (H, W, C). This is done because plt.imshow() expects the dimensions to be (H, W, C) but the tensor is of the shape (C, H, W).\n",
    "    plt.axis('off') # This is used to turn off the axis.\n",
    "    if show: \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    \"\"\"\n",
    "    Explanation:\n",
    "    This function is used to initialize the weights of the model. \n",
    "    If the module is a Conv2d or ConvTranspose2d layer, the weights are initialized using a normal distribution with mean 0 and standard deviation 0.02.\n",
    "    If the module is a BatchNorm2d layer, the weights are initialized using a normal distribution with mean 0 and standard deviation 0.02 and the bias is initialized to 0.\n",
    "\n",
    "    What is the use of nn.init.normal_ and nn.init.constant_?\n",
    "    - nn.init.normal_: This is used to initialize the weights of the module using a normal distribution.\n",
    "    - nn.init.constant_: This is used to initialize the bias of the module to a constant value.\n",
    "\n",
    "    What is bias in a neural network?\n",
    "    - Bias is a constant value that is added to the weighted sum of the inputs to the neuron. It allows the neuron to learn the optimal function that maps the input to the output.\n",
    "\n",
    "    Why are we making the bias zero?\n",
    "    - We are making the bias zero to ensure that the initial output of the neuron is close to zero. This helps in training the model faster as the gradients will not be too large.\n",
    "    \"\"\"\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.normal_(m.weight,0.0,0.02)\n",
    "            nn.init.constant_(m.bias,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_encoded_vector_for_labels(labels,num_classes):\n",
    "    \"\"\"\n",
    "    Explanation:\n",
    "    This function is used to convert the labels to one-hot encoded vectors.\n",
    "\n",
    "    What is the use of one-hot encoding and why are we using it here?\n",
    "    - One-hot encoding is used to represent categorical data as binary vectors. It is used to convert the labels to a format that can be used as input to the neural network. In this case, we are using it to convert the labels to a format that can be used as input to the generator.\n",
    "\n",
    "    For example:\n",
    "    x = torch.tensor([0, 1, 2, 1])\n",
    "    num_classes = 3\n",
    "\n",
    "    Result:\n",
    "    tensor([[1, 0, 0],\n",
    "            [0, 1, 0],\n",
    "            [0, 0, 1],\n",
    "            [0, 1, 0]])\n",
    "\n",
    "    What does this result mean?\n",
    "    - The first label is 0, so the one-hot encoded vector is [1, 0, 0].\n",
    "    - The second label is 1, so the one-hot encoded vector is [0, 1, 0].\n",
    "    - The third label is 2, so the one-hot encoded vector is [0, 0, 1].\n",
    "    - The fourth label is 1, so the one-hot encoded vector is [0, 1, 0].\n",
    "    \"\"\"\n",
    "    return torch.nn.functional.one_hot(labels,num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_vecs(x,y):\n",
    "    \"\"\"\n",
    "    Explanation:\n",
    "    This function is used to concatenate the noise vector x and the one-hot encoded vector y.\n",
    "\n",
    "    Why are we concatenating the noise vector and the one-hot encoded vector?\n",
    "    - We are concatenating the noise vector and the one-hot encoded vector to create a single input vector that can be used as input to the generator. This is done to generate images based on the noise vector and the class label.\n",
    "\n",
    "    For example:\n",
    "    x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "    y = torch.tensor([[7, 8], [9, 10]])\n",
    "\n",
    "    Result:\n",
    "    tensor([[ 1.,  2.,  3.,  7.,  8.],\n",
    "            [ 4.,  5.,  6.,  9., 10.]])\n",
    "\n",
    "    What does this result mean?\n",
    "    - The first row of the result is the concatenation of the first row of x and the first row of y.\n",
    "    - The second row of the result is the concatenation of the second row of x and the second row of y.\n",
    "\n",
    "    Why are converting x and y to float?\n",
    "    - We are converting x and y to float to ensure that the input to the generator is of the correct data type. The generator expects the input to be of type float.\n",
    "\n",
    "    Why are we concatenating along dim=1?\n",
    "    - We are concatenating along dim=1 to concatenate the vectors along the columns. This is done to create a single input vector that can be used as input to the generator.\n",
    "\n",
    "    Rules of concatenation:\n",
    "    - The dimensions of the input tensors should be the same except for the dimension along which they are concatenated.\n",
    "    - The dimensions of the input tensors along the concatenation dimension should be the same.\n",
    "    \"\"\"\n",
    "    return torch.cat((x.float(),y.float()),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "latent_dim = 64 # dimension of the noise vector\n",
    "img_channels = 1 # number of channels in the input image (1 for grayscale, 3 for RGB)\n",
    "num_classes = 10 # number of classes in the dataset (MNIST has 10 classes, from 0 to 9)\n",
    "lr = 2e-4 # learning rate\n",
    "batch_size = 128 # batch size for training the model\n",
    "num_epochs = 50 # number of epochs for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the generator and discriminator\n",
    "\n",
    "generator_latent_dim = latent_dim + num_classes # dimension of the input to the generator\n",
    "discriminator_img_channels = img_channels + num_classes # dimension of the input to the discriminator\n",
    "\n",
    "gen = Generator(latent_dim=generator_latent_dim,img_channels=img_channels).to(device)\n",
    "disc = Discriminator(img_channels=discriminator_img_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the weights of the generator and discriminator\n",
    "gen.apply(initialize_weights)\n",
    "disc.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = nn.BCEWithLogitsLoss() # Binary Cross Entropy with Logits Loss is used as the loss function\n",
    "\n",
    "# What is the meaning of Logits loss in PyTorch?\n",
    "# - Logits loss is a loss function that is used to calculate the loss between the predicted values and the actual values. It is used when the output of the model is not normalized and the values are not between 0 and 1.\n",
    "\n",
    "# Optimizers\n",
    "gen_opt = torch.optim.Adam(gen.parameters(),lr=lr,betas=(0.5,0.999)) # Adam optimizer is used for training the generator\n",
    "disc_opt = torch.optim.Adam(disc.parameters(),lr=lr,betas=(0.5,0.999)) # Adam optimizer is used for training the discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,),(0.5,))\n",
    "]) # Normalize the pixel values to be between -1 and 1\n",
    "\n",
    "mnist_data = datasets.MNIST(root='./data',train=True,download=True,transform=transform) # Load the MNIST dataset\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_data,batch_size=batch_size,shuffle=True) # Create a data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "current_step = 0\n",
    "generator_losses = []\n",
    "discriminator_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for real,labels in tqdm(data_loader):\n",
    "        current_batch_size = len(real)\n",
    "        real_images = real.to(device) # Move the real images to the device\n",
    "\n",
    "        # Convert the labels to one-hot encoded vectors\n",
    "        one_hot_encoded_labels = get_one_hot_encoded_vector_for_labels(labels,num_classes).to(device) # Convert the labels to one-hot encoded vectors\n",
    "    \n",
    "        # reshape the image labels to the shape of the input image which is (batch_size,num_classes,28,28) in this case for MNIST\n",
    "        img_one_hot_encoded_labels = one_hot_encoded_labels.view(current_batch_size,num_classes,1,1)\n",
    "        img_one_hot_encoded_labels = img_one_hot_encoded_labels.repeat(1,1,28,28) # repeat the labels along the height and width dimensions to match the shape of the input image\n",
    "        \"\"\"\n",
    "        Breaking down the above code:\n",
    "        - one_hot_encoded_labels.view(current_batch_size,num_classes,1,1): This reshapes the one-hot encoded labels to the shape (batch_size,num_classes,1,1).\n",
    "        - img_one_hot_encoded_labels.repeat(1,1,28,28): This repeats the one-hot encoded labels along the height and width dimensions to match the shape of the input image. The resulting shape will be (batch_size,num_classes,28,28).\n",
    "\n",
    "        1. Reshaping the Labels Tensor:\n",
    "        Initially, the labels tensor has a shape of (batch_size, num_classes), where batch_size is the number of samples in the batch, and num_classes is the number of classes or categories.\n",
    "        To make the labels tensor compatible with the image tensor, which typically has dimensions (batch_size, channels, height, width), we reshape the labels tensor to have dimensions (batch_size, num_classes, 1, 1).\n",
    "        This reshaping essentially adds two extra dimensions to the labels tensor, turning it into a 4D tensor where each label becomes a \"channel\" in the spatial dimensions.\n",
    "        \n",
    "        2. Repeating Along Height and Width:\n",
    "        Since the labels tensor has been reshaped to match the spatial dimensions of the image tensor, we now need to ensure that the labels are repeated across the entire spatial extent of each image.\n",
    "        To achieve this, we use the repeat() function to repeat the labels along the height and width dimensions of the image tensor.\n",
    "        For example, if the original images have spatial dimensions of 28x28, and we want each label to cover the entire spatial area of the corresponding image, we repeat the labels tensor 28 times along both the height and width dimensions.\n",
    "        This ensures that each pixel in the generated image is associated with the appropriate label, allowing the discriminator network to learn to distinguish between different classes at each spatial location.\n",
    "        \"\"\"\n",
    "\n",
    "        ##########################\n",
    "        # Train the discriminator #\n",
    "        ##########################\n",
    "\n",
    "        disc_opt.zero_grad()\n",
    "\n",
    "        # Generate noise vector for the current batch and concatenate it with the one-hot encoded labels\n",
    "        fake_noise = create_noise_vector(current_batch_size,latent_dim,device)\n",
    "        fake_noise_with_labels = concatenate_vecs(fake_noise,one_hot_encoded_labels)\n",
    "\n",
    "        # Get the fake images from the generator\n",
    "        fake_images = gen(fake_noise_with_labels)\n",
    "\n",
    "        # Check if the length of the fake images is the same as the batch size\n",
    "        assert len(fake_images) == len(real_images)\n",
    "\n",
    "        # Get the predictions for the fake images\n",
    "        # 1. Create input for the discriminator\n",
    "        disc_input_with_labels = concatenate_vecs(fake_images,img_one_hot_encoded_labels)\n",
    "        # 2. Get the predictions for the fake images\n",
    "        disc_fake_pred = disc(disc_input_with_labels.detach()) # detach() is used to remove the tensor from the computation graph. This is done to save memory. We dont need to update the weights of the generator here.\n",
    "        disc_real_pred = disc(concatenate_vecs(real_images,img_one_hot_encoded_labels))\n",
    "\n",
    "        # Calculate the loss for the discriminator\n",
    "        disc_fake_loss = criterion(disc_fake_pred,torch.zeros_like(disc_fake_pred))\n",
    "        disc_real_loss = criterion(disc_real_pred,torch.ones_like(disc_real_pred))\n",
    "        disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "\n",
    "        # Update the weights of the discriminator\n",
    "        disc_loss.backward()\n",
    "        disc_opt.step()\n",
    "\n",
    "        ######################\n",
    "        # Train the generator #\n",
    "        ######################\n",
    "        gen_opt.zero_grad()\n",
    "\n",
    "        # Get the predictions for the fake images\n",
    "        disc_fake_pred = disc(disc_input_with_labels)\n",
    "\n",
    "        # Calculate the loss for the generator\n",
    "        gen_loss = criterion(disc_fake_pred,torch.ones_like(disc_fake_pred))\n",
    "\n",
    "        # Update the weights of the generator\n",
    "        gen_loss.backward()\n",
    "        gen_opt.step()\n",
    "\n",
    "        # Print the losses\n",
    "        if current_step % 500 == 0:\n",
    "            print(f\"Epoch {epoch}: Step {current_step}: Generator loss: {gen_loss.item()}, Discriminator loss: {disc_loss.item()}\")\n",
    "            generator_losses.append(gen_loss.item())\n",
    "            discriminator_losses.append(disc_loss.item())\n",
    "\n",
    "            plot_images_from_tensor(fake_images[:25])\n",
    "            plot_images_from_tensor(real_images[:25])\n",
    "\n",
    "        current_step += 1\n",
    "\n",
    "\n",
    "\n",
    "# Save the model\n",
    "torch.save(gen.state_dict(),'generator.pth')\n",
    "torch.save(disc.state_dict(),'discriminator.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
